[{"Source":" Structureit","Age":"30+ days ago","JobTitle":null,"Url":"https://www.indeed.co.za","Salary":null,"Location":null,"Description":null},{"Source":" Visa","Age":"30+ days ago","JobTitle":" Director, Big Data Engineer, Sub-Saharan Africa Data Science...","Url":"https://www.indeed.co.za/rc/clk?jk=cbe24468bc5354dc&fccid=a3f737e511d9fc8c&vjs=3","Salary":null,"Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\nJob Description\r\n\r\n\r\nPosition Summary\r\n\r\nThe Director of Data Science is a lead Data Engineer role in the Sub-Saharan Africa (SSA) team. We are looking for an expert with deep expertise in data warehousing and can build large-scale data processing systems by using the latest database technologies. This is a Pan-regional position and plays a critical role in enabling the data platforms through which Data Scientists, Analysts, and BI Users drive solutions for our Visa clients. Also, the role provides a bridge between our local end-users and our Visa Technology colleagues in San Francisco, influencing the development of our global data platforms whilst provisioning local tools and technologies as required. The Data Engineer takes responsibility for building and running data pipelines, designing our local data warehouse and data frameworks, and catering for different data presentation techniques.\r\n\r\n\r\n Principal Responsibilities\r\nDesign local modifications to our global data architecture, including new tools and technologies where necessary to meet regional use-cases\r\n Provide direction to the development of bespoke, client-specific data sandboxes\r\n Create and maintain optimal data pipeline architecture(s), based on our Global Technology Stack\r\n Identify, design, and implement internal process improvements to provide greater scalability to our existing client solutions\r\n Develop custom-built packages and “glue code” to support the needs of Data Scientists across the region\r\n Work with broader business stakeholders to assist clients and consultants with their data and infrastructure needs\r\n\r\nDesign architecture\r\n Work with Visa’s Global Technology team to leverage our existing architecture to best effect, whilst identifying new and complementary tools and technology that will better enable our local solutions; serve as key contact and subject matter expert in working with the Visa Technology functions around the world (both global and regional)\r\n\r\n\r\n\r\nDesign sandbox architecture\r\n\r\nProvide SME support to the design and build of client-specific data sandboxes that may leverage the advantages of cloud technologies whilst ensuring strong security and privacy controls\r\n\r\nBuild data pipelines\r\n Build and operate stable, scalable data pipelines that cleanse, structure and integrate data sets into accessible formats for Data Scientists and other end-users, ensuring that testing and monitoring functions are performed appropriately\r\n\r\nSupporting external clients’ data architecture\r\n Provide Data Engineering expertise support to Visa’s select top-level clients, advising on how to implement, acquire, and improve their existing and planned data solutions. Such solutions can involve complex data integration from various sources (from an internal data warehouse, applicable VisaNet transaction data or third-party data) within the constraints of the client’s legal and regulatory limitations.\r\n\r\n\r\n\r\nImplement for scale\r\n Co-create and contribute to the design and deployment of scalable, high volume and real-time data solutions (dashboards, data feeds, and algorithms) running in production systems to ensure optimal functioning and sustainability of solutions built\r\n\r\n\r\n\r\nSupport Data Scientists\r\n Provide advisory and hands-on support to Data Scientists by providing quality assurance to teams writing poor-quality data queries on our queue, and developing custom-built packages and “glue code” that take algorithms into production\r\n\r\n\r\n\r\nServe as subject matter expert\r\n Provide support and advisory assistance to business stakeholders (client relationship managers, consultants, and other internal stakeholders) in framing potential use-cases, client engagements, and internal initiatives\r\n\r\n\r\nQualifications\r\n\r\n\r\nProfessional Experience\r\n\r\n\r\n 5 - 7 years' application development and support experience.\r\n Deep knowledge of distributed data architecture, commonly-used BI tools, and approaches/packages deployed for machine learning build\r\n Experience creating production software/systems and a proven track record of identifying and resolving performance bottlenecks for production systems.\r\n Experience in machine learning algorithm design, feature engineering, validation, prediction, recommendation, and measurement.\r\n Experience with complex, high volume, multi-dimensional data, as well as machine learning models based on unstructured, structured, and streaming datasets.\r\n Good understanding of the Payments and Banking Industry including aspects such as consumer credit, consumer debit, prepaid, small business, commercial, co-branded and merchant\r\n Experience planning, organising, and managing multiple large projects with diverse cross-functional teams\r\n Demonstrated ability to incorporate new techniques to solve business problems\r\n Demonstrated resource planning and delivery skills\r\n\r\n\r\n\r\n Technical Expertise\r\n\r\n\r\n Post Graduate Degree in Information Technology\r\n Qualification in Computer Science or Engineering ideal.\r\n Certification in Hadoop (Cloudera or Hortonworks) and Apache Spark.\r\n Working knowledge of Hadoop ecosystem and associated technologies, e.g., Apache Spark, MLlib, GraphX, iPython, sci-kit, and Pandas\r\n Advanced experience in writing and optimizing efficient SQL queries and Python scripts; Scala and C++ experience is ideal\r\n Deliver results within committed scope, timeline and budget\r\n Very strong people/project management skills and experience\r\n Ability to travel within CEMEA on short notice\r\n\r\n\r\n\r\n Business Experience\r\n Ability to travel within CEMEA on short notice\r\n Results-oriented with strong problem solving skills and demonstrated intellectual and analytical rigor\r\n Good business acumen with a track record in solving business problems through data-driven quantitative methodologies. Experience in payment, retail banking, or retail merchant industries is preferred\r\n Team oriented, collaborative, diplomatic, and flexible style\r\n Very detailed oriented, is expected to ensure highest level of quality/rigor in reports and data analysis\r\n Proven skills in translating analytics output to actionable recommendations and delivery\r\n Experience in presenting ideas and analysis to stakeholders whilst tailoring data-driven results to various audience levels\r\n Exhibits intellectual curiosity and a desire for continuous learning\r\n\r\n\r\n\r\n Leadership Competencies\r\n Exhibits intellectual curiosity and a desire for continuous learning\r\n Demonstrates integrity, maturity and a constructive approach to business challenges\r\n Role model for the organization and implementing core Visa Values\r\n Respect for the Individuals at all levels in the workplace\r\n Strive for Excellence and extraordinary results\r\n Use sound insights and judgments to make informed decisions in line with business strategy and needs\r\n Leadership skills include an ability to allocate tasks and resources across multiple lines of businesses and geographies. Leadership extends to ability to influence senior management within and outside Analytics groups\r\n Ability to successfully persuade/influence internal stakeholders for building best-in-class solutions\r\n Change management leadership\r\n\r\n Additional Information\r\n\r\n null"},{"Source":" Acuity Consultants","Age":"18 days ago","JobTitle":" BIG DATA DEVELOPER","Url":"https://www.indeed.co.za/rc/clk?jk=18680d22d246ddf2&fccid=36b7d079d13b5af5&vjs=3","Salary":" R900 000 - R1 000 000 a year","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nPermanent\r\n\r\n\r\nR900 000 - R1 000 000 a year\r\n\r\n\r\n\r\nThis is an excellent opportunity for a BIG DATA DEVELOPER to join an international SOFTWARE COMPANY and create DATA SOLUTIONS for Major Global CAPITAL MARKETS FINANCIAL INSTITUTIONS.\r\n This role is with an established Data Solutions & Software company operating in South Africa, UK, USA, New Zealand, Thailand, & Mauritius.\r\n\r\nBased in JOHANNESBURG this BIG DATA DEVELOPER role offers a salary of R900K – R1M/annum, with benefits on top.\r\n\r\nTHE COMPANY:\r\n Over their 15 year history this DATA SOLUTIONS SOFTWARE COMPANY has provided CAPITAL MARKETS Financial Institutions with BIG DATA ENGINEERING and CUSTOM DEVELOPMENT aligned to deploying technology and unlocking the potential of data - to gather, process, compare and analyse data from multiple sources, and uncover hidden insights to drive business advantage.\r\n This is a fast-growing international business using technology to solve complex financial data challenges in a simple way. Having grown across 5 continents, this business continues to enjoy success and is a recognised specialist in CAPITAL MARKETS.\r\n\r\nTHE ROLE:\r\n As BIG DATA DEVELOPER, your role will involve building and operating a content management platform for high profile big data projects that revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\n You'll be working closely within a team of developers distributed in London, South Africa and New Zealand.\r\n The role will involve building and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs. This includes data cleansing, aggregation, financial computations.\r\n\r\nTHE TEAM are diverse, smart, agile but laid-back who are passionate about technology, open-minded and open to new ideas. In this business you’ll find serious hardware, and no red tape or unnecessary process. Not only will you get to work with a great team, you’ll also enjoy flexible hours, the flexibility to work from home when needed, private medical and a relaxed dress code.\r\n\r\nREQUIRED SKILLS:\r\n Ability to pick up a new technology quickly and deliver features in a highly agile manner.\r\n Experience writing functional Scala in a production grade system. (Not a Java developer writing OO in Scala).\r\n To have used Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\n A clear and practical understanding of how Hive works, which includes running Hive on Tez.\r\n Practical experience using Debian flavoured Linux distributions.\r\n Familiarity with event driven development and architecture.\r\n Used Docker containers to deploy your systems.\r\n Ability to navigate the administration of an HDP cluster on AWS.\r\n Able to index millions of documents from Hadoop into Elasticsearch.\r\n Work with various messaging systems, such as Kafka and RabbitMQ.\r\n Able to aggregate data using Apache Kylin Cube.\r\n Can pick up Python if you have not used it previously.\r\n Operate and deploy to a Kubernetes cluster on AWS.\r\n Understand basic concepts about mortgage backed securities.\r\n\r\nIf you qualify for this role, please email your CV directly to:\r\n Gary Silbermann\r\n gary@acuityconsultants.co.za\r\n 021 801 5001\r\n\r\nIf you have not had a response to your application within 14 days please consider your application to be unsuccessful."},{"Source":" Parvana Strategic Sourcing","Age":"30+ days ago","JobTitle":" Big Data Developer (CPT)","Url":"https://www.indeed.co.za/rc/clk?jk=960548b153c061f5&fccid=6ab9a95a1ff7d948&vjs=3","Salary":null,"Location":"Cape Town, Western Cape","Description":"\r\n\r\n\r\nCape Town, Western Cape\r\n\r\n\r\nPermanent\r\n\r\n\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\n\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\n\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk \r\n\r\n\r\n\r\nBig Data Developer (CPT) (New) | (1002333)\r\n\r\n[Permanent | Competitive Salary | Cape Town]\r\n\r\n\r\nClient Background: Our client develops and supports software and data solutions across a variety of industries. \r\n They want you to get ahead of the market and stay there. They offer a combination of plug and play products that can be integrated with existing systems and processes and can also be customised to client needs. \r\n Their capabilities extend to big data engineering and bespoke software development, solutions are available as both cloud-based and hosted.\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk"},{"Source":" Parvana Strategic Sourcing","Age":"30+ days ago","JobTitle":" Big Data Developer (JHB)","Url":"https://www.indeed.co.za/rc/clk?jk=dc2adc6e2281aaed&fccid=6ab9a95a1ff7d948&vjs=3","Salary":null,"Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nPermanent\r\n\r\n\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\n\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\n\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk \r\n\r\n\r\n\r\nBig Data Developer (JHB) (New) | (1002332)\r\n\r\n[Permanent | Competitive Salary | Johannesburg]\r\n\r\n\r\nClient Background: Our client develops and supports software and data solutions across a variety of industries. \r\n They want you to get ahead of the market and stay there. They offer a combination of plug and play products that can be integrated with existing systems and processes and can also be customised to client needs. \r\n Their capabilities extend to big data engineering and bespoke software development, solutions are available as both cloud-based and hosted.\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk"},{"Source":" PRR Recruitment Services","Age":"30+ days ago","JobTitle":null,"Url":"https://www.indeed.co.za","Salary":" R80 000 - R90 000 a year","Location":null,"Description":null},{"Source":" AWCA Human Capital","Age":"30+ days ago","JobTitle":" Data Warehouse Developer Lead","Url":"https://www.indeed.co.za/rc/clk?jk=4626685e77dee4a2&fccid=e87d374965cc1f57&vjs=3","Salary":" R80 000 a year","Location":"Western Cape","Description":"\r\n\r\n\r\nWestern Cape\r\n\r\n\r\nR80 000 a year\r\n\r\n\r\nREQUIREMENTS\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nIntimate knowledge of Microsoft SQL Server (required)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nGood understanding and knowledge of SSIS, SSRS and SSAS\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSSIS: Designing, Implementation and testing of ETL packages (required)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSSRS: Designing and development of Reports (required)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSSAS: Design, implementation and testing of Tabular and Multi-Dimensional Cubes (highly advantageous)\r\n\r\n\r\n\r\n\r\n\r\n\r\nKnowledge of Oracle database is advantageous and Power BI knowledge is preferred\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nFamiliar with agile development process\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPrevious experience working with Hadoop, Azure &/or Apache Spark and 'BigData'\r\n\r\n\r\n\r\n\r\nRESPONSIBILITES\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nDevelop and deploy full Data warehousing strategy for the company.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCo-ordinate and develop data integration for all third-party systems\r\n\r\n\r\n\r\n\r\n\r\n\r\nLiaise with business with respect to current and future data requirements\r\n\r\n\r\n\r\n\r\n\r\n\r\nManage a team of SQL and Reports Developers"}]