[{"JobTitle":" Data Engineer â€† BIG DATA - Johannesburg North â€† Up to R9...","Url":"https://www.indeed.co.za/pagead/clk?mo=r&ad=-6NYlbfkN0AMOOo83f3x9FLd22RLK3YfFusStXJLn5R4tAT9NKF0tAvdVEybpYDI30GAsFF9juayDBn6dJZ1w7d_b9MOcJSkVRRGdkMPQ-sNKOrHOdXrZCKXveOIsZOyWD90376nitjkbe6LT7n1fxifgX18wGNBHpAX9b4J1R4l3p2GpPPBocOYa1S93NEz5hQ8MKrHctpI_Ci5BEuknz1ZUsgsnfH56l0S8E6BZAIcEM_IlwvYtlmlakOjYNkd9_lZDCORgtnhQZFIUesgYk19ywlPvt9gANXXHZwwpCiUMjqp3WUMXpo1jwOa964Xps9idDueZTCnGHbQwLROTsjSbLc7_lhdVVXucDtizgLRtrdhMjXqFa492Sw2MnHPbZRJxFw9QWCdmwIllk5ux3e8LGH5HbUm64U01y5_aI8WxroYJzIlzVU8YEVc0VyY6YQyMktNK7YgLM39VuEF4gyIyFfXwGJXSIwXxguwchYoLi8q79nHqgz9N1CS3TvkAwCVykc8dwCaZDR4JVQhL46TM8gtBSD42JypXSRkAFj9Aux5VRn6hcjeG6B96LXHAzmH-1mr_YpYMOHNqRVBr7uvebWEPkmQMrnNe2UqJnj5BoQiF7GYYQ==&p=0&fvj=0&vjs=3","Salary":" R900 000 a year","Source":" E-Merge","Age":null,"Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nR900 000 a year\r\n\r\n\r\nThis is an opportunity to work on cutting edge of tech and cloud computing. They are a fun environment with a quality team that is working with some of the biggest and best datasets in the country.\r\n\r\nYou will be building world-class, scalable data pipelines and warehouses to add real value to customers and drive new product development, both internally and externally.\r\n\r\n Requirements:\r\n Completed BSc (Computer Science) degree or similar (Software Engineering, Data Science etc. with experience in software)\r\n 2-5 years’ experience in Data Engineering (Hadoop and Spark)\r\n Data Processing products - Big Query, Redshift, Spectrum, S3, Athena, Kafka, Spark, Storm, Flink, Beam, Presto, Hive\r\n ETL processes and transformations\r\n Cloud experience ideally with Google Cloud Platform\r\n DevOps Stack development experience\r\n Apache-Airflow or other data pipeline tools\r\n Exposure to Scala or Java in context of data processing\r\n Experience and proficiency with Python\r\n Experience in the design and implementation of data flows\r\n Advanced SQL/PostgreSQL/Redshift knowledge\r\n\r\n Responsibilities:\r\n Support sophisticated predictive data products by maintaining data science production environments (cloud-based, python), ensuring that the outputs from Data Science models are available and integrated into the system and integrate, coordinate and maintain data flows between various sources of data\r\n Manage and maintain cloud service integrations that perform key data functions by working towards replacing third-party elements of the data pipeline by using open-source tools\r\n Make decisions around the infrastructure, layout and processes of the data warehouse, including:\r\n working with the engineering team on how to best track and record data\r\n following up on data inconsistencies to ensure that it is corrected\r\n Transforming, standardizing and collecting data from various sources\r\n\r\n Reference Number for this position is LV46312 which is a permanent position based in Johannesburg North offering a salary of up to R900k per annum highly negotiable on experience and ability. Contact Liza on <-Please upload your CV here- or call her on<-Please upload your CV here- to discuss this and other opportunities.\r\n\r\nAre you ready for a change of scenery? The e-Merge IT recruitment is a specialist niche recruitment agency. We offer our candidates options so that we can successfully place the right developers with the right companies in the right roles.\r\n\r\nCheck out the e-Merge website <-Please upload your CV here- for more great positions\r\n\r\nDo you have a friend who is a developer or technology specialist? We pay cash for successful referrals!"},{"JobTitle":" BIG Data Architect","Url":"https://www.indeed.co.za/rc/clk?jk=4b5fc168693f01f6&fccid=a4e4e2eaf26690c9&vjs=3","Salary":null,"Source":" Accenture","Age":"30+ days ago","Location":"South Africa","Description":"\r\n\r\n\r\nSouth Africa\r\n\r\n\r\nAligning technology with business strategy and goals they work directly with the client gathering requirements to analyze, design and/or implement technology best practice business changes. They are sought out as experts internally and externally for their deep functional or industry expertise, domain knowledge, or offering expertise. They enhance Accenture's marketplace reputation.\r\n\r\n\r\n\r\n Innovate with the newest technologies. \r\n\r\nBe part of the New. Now.\r\n\r\nUnleash your skills and transform the world around you. Shape the technology of tomorrow, with the latest digital methodologies and design thinking, like cloud, AI, intelligent automation, DevOps and Agile. Implement innovative solutions to help clients drive disruption and stay ahead of the digital curve. \r\n\r\nDeveloping, designing and maintaining technologies that improve the way our clients and the world works. Supporting the core of Accenture's Technology business, they use curiosity to solve Technology problems through developing, designing, and maintaining software products or systems that enable client strategies. Working in challenging and dynamic environments, they use their versatility to create and support technology solutions that meet client requirements from analysis to implementation. \r\n\r\nJoin Accenture and help transform leading organizations and communities around the world. The sheer scale of our capabilities and client engagements and the way we collaborate, operate and deliver value provides an unparalleled opportunity to grow and advance. Choose Accenture, and make delivering innovative work part of your extraordinary career. \r\n\r\nPeople in our Client Delivery & Operations career track drive delivery and capability excellence through the design, development and/or delivery of a solution, service, capability or offering. They grow into delivery-focused roles, and can progress within their current role, laterally or upward.\r\n\r\n\r\n\r\n Our offer\r\nUnparalleled opportunities to build a career \r\nComprehensive and specialized training covering business acumen, digital acumen, technical and professional skills development \r\nCareer development opportunities and perspective \r\nChallenging and exciting projects at some of the world's leading companies in all industries \r\nAccess to a global network of deeply skilled and experienced supply chain professionals\r\n\r\nJob Description \r\n\r\nWe are looking for a Big Data Architect and Team Lead who has a passion and interest to help organizations optimize their business performance by ingesting, processing, and managing big volumes of fast multi structured data as well as extracting meaningful value from these large volumes of data. You will have the knowledge and previous experience of using Big Data methodologies, solutions and platforms and tools to enable you to help our clients.\r\n\r\n\r\n\r\nYou will be expected to be able to work independently and, in a team, and have experience working in an agile environment. We are looking for someone who is a passionate architect and you must have the ability to articulate your ideas and work well with our clients at the architectural level. You will need to have strong communication skills and be comfortable and confident engaging and leading our team members and key client stakeholders. You will have also worked across multiple platforms such as Cloudera, Hortonworks, Google Cloud, Azure and AWS and have previous experience of building Enterprise Data Lakes. Additional experience in metadata driven data ingestion, Big data metadata management, governance and implementing data security controls would be advantageous.\r\n\r\n\r\n\r\nYour Experience\r\n\r\nAs a Big Data Architect, you will be responsible for guiding the full lifecycle of a Big Data solution, including:\r\nFocus on designing and implementing Big Data Pipelines \r\nManaging and engaging with key client stakeholders and leading an internal team of developers and engineers \r\nTake ownership for end to end solution integrity (including design, build, test and deploy) \r\nWork with and guiding clients to ensure the best application of big data solutions is chosen for their current situation \r\nEffectively communicate with clients to contribute throughout the end-to-end delivery lifecycle of complex and large-scale Big Data lakes \r\nMaintain and communicate the current-state architecture (where we are today) and the target-state architecture vision (where we are going) \r\nSupport in Proof of concept execution to prove the value of Big Data use cases \r\nHelp facilitate the full lifecycle implementation from requirements analysis, platform selection, technical architecture design, application design and development, testing, and deployment \r\nReview of all technical approaches for platform enhancements and customer solutions \r\nTrack and monitor progress of tasks and proactively raise issues which may impact delivery \r\nSupport the translation of the technical architecture into backlog generation, estimation exercises for sprint planning and future release planning and the review and signoff of all technical deliverables\r\n\r\n\r\n\r\nOur Expectations include the ability and experience to perform the following:\r\n3-5 years of hands on experience across at least 2 Big Data project implementations \r\nAbility to articulate technology choices in addressing architecture concerns and constraints \r\nAt least 5 years of consulting experience having successfully led large teams in delivering customer outcomes \r\nUnderstanding of DevOps pipelines and key Agile concepts \r\nKnowledge of cloud infrastructure (particularly AWS or Azure) \r\nKnowledge of security design, data management, data governance, lineage and audit. \r\nCan demonstrate an understanding across systems integration, information management, data management and architecture, and business analytics \r\nHas deep experience with Cloudera Hadoop (or similar stack such as Hortonworks) distributions and application programming (MapReduce, Pig, Cascading, Hive, Impala, Storm, Spark etc.) \r\nExposure to working in Agile projects with involvement in MVP definition, sprint planning, back-log grooming ad resolving blockers on technical solutions \r\nExperience leading large-scale, multi-release information centric projects and delivery experience leveraging multiple delivery locations (onshore + nearshore/offshore) \r\nFamiliar with designing high volume and fast big data solutions that have been successfully implemented into a production environment, both batch and stream processing using frameworks like Apache Nifi/Beam/Flink \r\nStrong understanding of data warehouse, data lake and associated ETL and data processing concepts \r\nAbility to articulate and hold solution discussion with key stakeholders Senior Enterprise Analytics Architects \r\nUniversity degree, preferably in Computer Science/IT/Engineering or in a related field \r\nDemonstrate a passion for innovation and new idea generation"},{"JobTitle":null,"Url":"https://www.indeed.co.za","Salary":null,"Source":" Structureit","Age":"30+ days ago","Location":null,"Description":null},{"JobTitle":" Big Data Platform Engineer(Hadoop)","Url":"https://www.indeed.co.za/rc/clk?jk=aca8b3c9b0a849a0&fccid=b54c24d0673e4b40&vjs=3","Salary":null,"Source":" Praesignis","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nContract\r\nIntermediate Big Data Platform Engineer (Hadoop) responsible for the enterprise-wide administration and engineering of Hadoop and Data Science Workbench environments based in Gauteng. \r\n\r\n\r\n\r\n\r\n\r\nSystem and database Administration\r\nSolution and infrastructure, design, development, intergration, security, networking and automation\r\nData management & governance, user support and education\r\nCloud\r\nHadoop Environment\r\nLinux(Ubuntu,SLES,CentOS/RH)\r\nLog Debugging Enthusiast\r\nHive Maintanance\r\n\r\n\r\n\r\n\r\nAd visible until: 4 August 2019"},{"JobTitle":" Data Engineer","Url":"https://www.indeed.co.za/rc/clk?jk=57e026e661271eea&fccid=879a62b6121692b6&vjs=3","Salary":null,"Source":" OfferZen","Age":"30+ days ago","Location":"Cape Town, Western Cape","Description":"\r\n\r\n\r\nCape Town, Western Cape\r\nWhy is now a good time to join OfferZen? \r\n\r\n Over the last 3 years, OfferZen ( https://www.offerzen.com/ ) has managed to make a large dent in the South African tech recruitment space – and we're speeding up. Our team has exploded from 4 to 60+. We're looking to add more smart people who want to contribute meaningfully. \r\nOfferZen was founded by Philip and Malan Joubert, along with other successful tech startups like SnapScan ( http://www.snapscan.co.za/ ), JourneyApps ( https://www.journeyapps.com/ ) and Luno ( https://www.luno.com/ ). \r\nOur mission is to improve the experience of finding work for makers – people involved in making software, from developers to designers and data scientists. OfferZen does that by flipping the traditional recruitment model on its head and getting companies to reach out to makers with upfront offers. \r\n\r\n The opportunity \r\n\r\n You'll be working in a Data squad as part of our Product Group, a talented group of developers, engineers and data scientists focussing on our data platform. You'll be joining a growing, data-centric organization with a culture of experimentation and analytical thinking. \r\n\r\n What you'll do \r\n\r\n At OfferZen, data is key to enabling teams, processes and our customer base to scale. You'll be helping create data products in three main areas: \r\n\r\nPersonalization – creating an excellent user experience for companies and candidates\r\nOperations – informing and assisting company level decision-making\r\nCustomer Growth – helping more people find an awesome tech job.\r\nThis creates an ideal opportunity for a Data Engineer to thrive and deliver high impact solutions. This position is well-suited to someone looking to grow their technical and collaboration skills. You'll work in a supportive environment, with outstanding teammates, and gain experience across the full range of responsibilities. \r\n\r\n What you'll need \r\n\r\n3+ years experience with data modelling and database technologies.\r\nA solid foundation in computer science in areas such as software architecture and data structures\r\nAbility to work both autonomously, and in a small team to solve complex problems\r\nCare about the quality and impact of your work\r\nA good knowledge of a high-level programming language (E.g. Ruby, Python, Go).\r\nExperience in using data processing for analytics or built data processing capabilities.\r\nExperience with database technologies and paradigms – relational, noSQL, warehousing\r\nBonus for experience with Hadoop based technologies (MapReduce, Hive, Pig)\r\nBEng or BSc CS degree, or equivalent working experience\r\nWhat it's like to work here \r\nWe are a fun loving and very driven team of 60+ people. We are transparent with our numbers and strategy and have weekly learning sessions where someone in the company presents something they do. We get lunch every day and have 20 days of leave a year. We do fun stuff together like going on picnics at Kirstenbosch, playing board games and having Sumo wrestling events! \r\n\r\n About OfferZen \r\nWe're helping makers – the people who design and build software products – to unlock their individual potential. We started with recruitment because helping someone find a job they love is one of the fastest ways of changing their life. Since then, we've started expanding to things like training, root ( https://root.co.za/ ) (programmable banking for developers) and building tools for the tech community. \r\n\r\nour team : )"},{"JobTitle":" Senior Data Engineer","Url":"https://www.indeed.co.za/rc/clk?jk=2ac4dd802f3e65b3&fccid=745f50042cd32f08&vjs=3","Salary":null,"Source":" Unique Personnel","Age":"5 days ago","Location":"South Africa","Description":"\r\n\r\n\r\nSouth Africa\r\n\r\n\r\nContract\r\n\r\n\r\n\r\n\r\n\r\nJob Listing \r\n\r\n\r\n\r\n\r\n\r\nJob Number\r\n\r\n58073 \r\n\r\n\r\n\r\n\r\nJob Type\r\n\r\nContract \r\n\r\n\r\n\r\n\r\nJob Title\r\n\r\nSenior Data Engineer \r\n\r\n\r\n\r\n\r\nComputer Skills\r\n\r\nData Analytics,Database Administration,Java,MS Excel,MS Outlook,MS PowerPoint,MS SQL,MS Word,MySQL,Python,SQL,Windows 10 \r\n\r\n\r\n\r\n\r\nIndustry\r\n\r\nIT Development & Software \r\n\r\n\r\n\r\n\r\nCity\r\n\r\nPlease select city \r\n\r\n\r\n\r\n\r\nProvince\r\n\r\nGauteng \r\n\r\n\r\n\r\n\r\nJob Description\r\n\r\nThe Senior Data Engineer is responsible for overseeing junior data engineering activities and aiding in building the business’ data collection systems and processing pipelines. The role of the Senior Data Engineer is responsible for building and maintaining optimized and highly available data pipelines that facilitate deeper analysis and reporting by the Data and Analytics department. The Senior Data Engineer builds data processing frameworks that handle the business’s growing database. He works with senior data science leadership as well as other Data and Analytics teams in leveraging data with reporting and scientific tools, for example, Tableau, R, and Spark. The Senior Data Engineer strives to continuously develop new and improved data engineering capabilities. Management and Strategy: The managerial role of the Senior Data Engineer is primarily for overseeing activities of the junior data engineering teams, ensuring proper execution of their duties and alignment with business vision and objectives. He provides senior-level contribution to a team that is responsible for the design, deployment, and maintenance of the business’s data platforms. However, the Senior Data Analyst will also implement strategies directed at acquiring data and promoting the development of new insights across the business. The Senior Data Engineer owns and extends the business’s data pipeline through the collection, storage, processing, and transformation of large data-sets. It is his duty to monitor the existing metrics, analyze data, and lead partnership with other Data and Analytics teams in an effort to identify and implement system and process improvements. The Senior Data Engineer will additionally develop queries for ad hoc business projects, as well as ongoing reporting. In this capacity, the Senior Data Engineer builds a metadata system where all available data is maintained and cataloged. The Senior Data Engineer also plays a major role in the development of reliable data pipelines that translate raw data into powerful features and signals. He designs, architects, implements, and supports key datasets that avail structured and timely access to actionable business insights. The Senior Data Engineer is additionally tasked with developing ETL processes that convert data into formats through a team of data analysts and dashboard charts. Collaboration and Support: The Senior Data Engineer plays a collaborative role where he works closely with the business’s Data and Analytics teams, gathering technical requirements for exceptional data governance across the department and the business at large. In this collaboration, the Senior Data Engineer works the data analysts, data warehousing engineers, and data scientists in finding and applying best practices within the Data and Analytics department as well as defining the business’s data requirements, which will ensure that the collected data is of a high quality and optimal for use across the department and the business at large. The Senior Data Engineer will also work with senior data science management and departments beyond the Data and Analytics department in analyzing and understanding data sources, participating in design, and providing insights and guidance on database technology and data modeling best practices. In this capacity, the Senior Data Engineer will further be required to draw performance reports and strategic proposals form his gathered knowledge and analyses results for senior data science leadership. Analytics: The Senior Data Engineering plays an analytical role where he develops and manages scalable data processing platforms that he uses for exploratory data analysis and real-time analytics. It is also the role of the Senior Data Engineer to oversee, design, and develop algorithms for real-time data processing within the business and to create the frameworks that enable quick and efficient data acquisition. In this capacity, the Senior Data Engineer retrieves and analyzes data through the use of SQL, Excel, among other data management systems. He also builds data loading services for the purpose of importing data from numerous disparate data sources, inclusive of APIs, logs, relational, and non-relational databases. Knowledge and Opportunity: The Senior Data Engineer is tasked with the responsibility of contributing to the continual improvement of the business’s data platforms through his observations and well-researched knowledge. He keeps track of industry best practices and trends and through his acquired knowledge, takes advantage of process and system improvement opportunities. Other Duties: The Senior Data Engineer performs similar duties as he deems fit for the proper execution of his duties and duties as delegated by the Head of Data Science, Director Data Science, Chief Data Officer, or the Employer. Experience: A candidate for this position must have had at least 5 years of working experience working in a data engineering department, preferably as a Data Engineer in a fast-paced environment and complex business setting. The candidate must have a demonstrated experience in building and maintaining reliable and scalable ETL on big data platforms as well as experience working with varied forms of data infrastructure inclusive of relational databases such as SQL, Hadoop, Spark and column-oriented databases such as Redshift, MySQL, and Vertica. The candidate must also have had experience in data warehousing inclusive of dimensional modeling concepts and demonstrate proficiency in scripting languages, for example, Python, Perl, and so forth. A suitable candidate will also demonstrate machine learning experience and experience with big data infrastructure inclusive of MapReduce, Hive, HDFS, YARN, HBase, Oozie, etc. The candidate will additionally demonstrate substantial experience and a deep knowledge of data mining techniques, relational, and non-relational databases. Communication Skills: Communication Skills for the Senior Data Engineer are just as important as they are for the Data Engineer, both in verbal and written form. The Senior Data Engineer oversees and manages junior data engineering teams and to ensure effective management, he must be capable of conveying information and instructions clearly down the line to the junior team. Communication skills are also imperative for the Senior Data Engineer in his collaborative role where he will have to interact cross-functionally with non-technical departments. To enable effective collaborations, the Senior Data Engineer will have an exceptional ability to convey complex messages in a clear, simplified, and understandable manner. He will also be required to draft reports and prepare presentations for senior data science leadership. These reports and presentation must be clear, concise, unambiguous, engaging and convincing, which will demand exceptional communication skills on the Senior Data Engineer’s part. Ms Office/Software: A candidate for this position must be highly proficient in the use of Ms Word, Ms Excel, PowerPoint, and Outlook, which will all be necessary for the creation of both visually and verbally engaging reports and presentations, for senior data science leadership. The Senior Data Engineer must further have exceptional skills in SQL server reporting services, analysis services, Tableau, Salesforce, integration services, or any other data visualization tools. Technological Savvy/Analytical Skills: A candidate for this position will also demonstrate strong computer skills and a deep passion for analytics. The candidate for this position must possess an ability to perform complex data analyses with large data volumes. He will be an expert in SQL, Java, and have a keen understanding of data models and data warehouse concepts. The candidate will demonstrate an ability translate algorithms provided by senior data science management and implement them in as well as strong knowledge in Linux, OS tools, and file-system level troubleshooting. The candidate must have substantial experience working with big data infrastructure tools such as Python, SQS, and Redshift. A suitable candidate will also be proficient Scala, Spark, Spark Streaming, AWS, and EMR. Interpersonal Skills: The Senior Data Engineer must have certain preferable personal attributes that will make him that much more suited for the position. The Senior Data Engineer will be a result-driven individual, be passionate and a self-starter, be proactive requiring minimal supervision, be highly organized, have an ability to handle multiple tasks and meet tight deadlines, be a creative and strategic thinker, work comfortably work in a collaborative setting, work comfortably with senior departmental leadership, and demonstrate an ability to remain calm during times of uncertainty and stress, inspiring the same in his team. People Skill: The candidate must be a people person who is able to form strong, meaningful, and lasting connections with others, enabling smooth and continued collaborative relationships, earning him the trust of his juniors who will readily follow in his directives, and gaining the confidence of senior data science leadership \r\n\r\nTo apply immediately for this position click here: http://www.unique.co.za/candidate_registration_1.aspx?JobID=58073&referrer=Unique\r\n\r\n\r\n\r\n\r\nInherent Requirements\r\n\r\nThe Senior Data Engineer must have a bachelor’s degree (masters’ preferred) in Computer Science, Applied Mathematics, Engineering, or any other technology related field. An equivalent of this educational requirement in working experience is also acceptable. \r\n\r\n\r\n\r\n\r\nWhat Qualification\r\n\r\nRelated to Industry \r\n\r\n\r\n\r\n\r\nMinimum Qualification\r\n\r\nMasters \r\n\r\n\r\n\r\n\r\nStatus\r\n\r\nAvailable"},{"JobTitle":" Data Engineer","Url":"https://www.indeed.co.za/rc/clk?jk=fb1110db122115d3&fccid=77748257c144323d&vjs=3","Salary":" R900 000 a year","Source":" e-Merge IT Recruitment","Age":"10 days ago","Location":"Johannesburg North, Gauteng","Description":"\r\n\r\n\r\nJohannesburg North, Gauteng\r\n\r\n\r\nR900 000 a year\r\n\r\n\r\nDescription\r\n\r\n\r\nA data centered business is on the lookout for a Data Engineer to join their team. This is an opportunity to work on cutting edge of tech and cloud computing. They are a fun environment with a quality team that is working with some of the biggest and best datasets in the country.\r\n\r\nYou will be building world-class, scalable data pipelines and warehouses to add real value to customers and drive new product development, both internally and externally.\r\n\r\n Requirements:\r\n Completed BSc (Computer Science) degree or similar (Software Engineering, Data Science etc. with experience in software)\r\n 2-5 years’ experience in Data Engineering (Hadoop and Spark)\r\n Data Processing products - Big Query, Redshift, Spectrum, S3, Athena, Kafka, Spark, Storm, Flink, Beam, Presto, Hive\r\n ETL processes and transformations\r\n Cloud experience ideally with Google Cloud Platform\r\n DevOps Stack development experience\r\n Apache-Airflow or other data pipeline tools\r\n Exposure to Scala or Java in context of data processing\r\n Experience and proficiency with Python\r\n Experience in the design and implementation of data flows\r\n Advanced SQL/PostgreSQL/Redshift knowledge\r\n\r\n Responsibilities:\r\n Support sophisticated predictive data products by maintaining data science production environments (cloud-based, python), ensuring that the outputs from Data Science models are available and integrated into the system and integrate, coordinate and maintain data flows between various sources of data\r\n Manage and maintain cloud service integrations that perform key data functions by working towards replacing third-party elements of the data pipeline by using open-source tools\r\n Make decisions around the infrastructure, layout and processes of the data warehouse, including:\r\n working with the engineering team on how to best track and record data\r\n following up on data inconsistencies to ensure that it is corrected\r\n Transforming, standardizing and collecting data from various sources\r\n\r\n Reference Number for this position is LV46312 which is a permanent position based in Johannesburg North offering a salary of up to R900k per annum highly negotiable on experience and ability. Contact Liza on lizavdb@e-merge.co.za or call her on 011 463 3633 to discuss this and other opportunities.\r\n\r\nAre you ready for a change of scenery? The e-Merge IT recruitment is a specialist niche recruitment agency. We offer our candidates options so that we can successfully place the right developers with the right companies in the right roles.\r\n\r\nDo you have a friend who is a developer or technology specialist? We pay cash for successful referrals!"},{"JobTitle":" Partnerships Manager (South Africa)","Url":"https://www.indeed.co.za/rc/clk?jk=741233500db4f9cf&fccid=3df6032cb81491f6&vjs=3","Salary":null,"Source":" Honey","Age":"30+ days ago","Location":"Cape Town, Western Cape","Description":"\r\n\r\n\r\nCape Town, Western Cape\r\nHoney is a fast-growing startup based in Los Angeles. Our online shopping platform offers users a smarter way to shop. Through a simple browser extension, we open up instant access to exclusive savings, deals, rewards and discovery, all powered by the collective knowledge of Honey's community of online shoppers. We are helping millions save when they shop online, and we're hiring! We're expanding to new markets such as South Africa and are actively seeking a Partnerships Manager to join our Partnerships Team and grow our Cape Town office. \r\n\r\nAbout the Team:\r\n Our team's goal is to maximize partner sales and commissions without sacrificing product quality or straying from the company vision. We work closely with Tech, Product, Growth and the rest of the company to accomplish this. We are the public face of Honey and this role is social because it is an opportunity to efficiently convey everything that Honey has to offer to new and current partners. \r\n\r\nWhat You'll Do:\r\nAs a Partnerships Manager at Honey, you will:\r\nManage all aspects of performance reporting for the Merchants in your portfolio\r\nLead efforts in digging into partner performance. Regularly monitor KPIs and other metrics to ensure all campaigns are fully optimized\r\nTake the lead working with other departments (mainly System Quality, Product, Growth) to ensure partners are fully supported and tracked\r\nWork with your paired Director to identify new and existing partners opportunities and to make business decisions based on competitor landscape, seasonality and partner overall marketing goals\r\nIdentify Key Targets for you and your paired Director to pursue. Collect information and put together a plan before engaging.\r\nMaintain strong relationships with affiliate program manager at CJ, Rakuten, PJ, IR, OPM's, etc. and at the merchants directly.\r\nAdd, Screen, Edit and Test all codes, promotional phrasing and links regularly to ensure we deliver the best user experience and receive correct attribution for sales that we drive. AKA \"Shake the Hive\"\r\nWork with Content Specialists to identify programs that need optimization\r\nAbout You:\r\n4+ years of relevant experience (online marketing, sales/account management, performance marketing, ad tech)\r\nExcellent written, verbal and in-person communication skills\r\nA network of retailers, agency and networks reps who know and respect your work history\r\nAn analytical mindset\r\nProblem finding & solving attitude\r\nHoney is an equal opportunity employer. We are committed to building a diverse and inclusive company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, disability status or genetic information, in compliance with applicable federal, state and local law."},{"JobTitle":" BIG DATA DEVELOPER","Url":"https://www.indeed.co.za/rc/clk?jk=18680d22d246ddf2&fccid=36b7d079d13b5af5&vjs=3","Salary":" R900 000 - R1 000 000 a year","Source":" Acuity Consultants","Age":"18 days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nPermanent\r\n\r\n\r\nR900 000 - R1 000 000 a year\r\n\r\n\r\n\r\nThis is an excellent opportunity for a BIG DATA DEVELOPER to join an international SOFTWARE COMPANY and create DATA SOLUTIONS for Major Global CAPITAL MARKETS FINANCIAL INSTITUTIONS.\r\n This role is with an established Data Solutions & Software company operating in South Africa, UK, USA, New Zealand, Thailand, & Mauritius.\r\n\r\nBased in JOHANNESBURG this BIG DATA DEVELOPER role offers a salary of R900K – R1M/annum, with benefits on top.\r\n\r\nTHE COMPANY:\r\n Over their 15 year history this DATA SOLUTIONS SOFTWARE COMPANY has provided CAPITAL MARKETS Financial Institutions with BIG DATA ENGINEERING and CUSTOM DEVELOPMENT aligned to deploying technology and unlocking the potential of data - to gather, process, compare and analyse data from multiple sources, and uncover hidden insights to drive business advantage.\r\n This is a fast-growing international business using technology to solve complex financial data challenges in a simple way. Having grown across 5 continents, this business continues to enjoy success and is a recognised specialist in CAPITAL MARKETS.\r\n\r\nTHE ROLE:\r\n As BIG DATA DEVELOPER, your role will involve building and operating a content management platform for high profile big data projects that revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\n You'll be working closely within a team of developers distributed in London, South Africa and New Zealand.\r\n The role will involve building and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs. This includes data cleansing, aggregation, financial computations.\r\n\r\nTHE TEAM are diverse, smart, agile but laid-back who are passionate about technology, open-minded and open to new ideas. In this business you’ll find serious hardware, and no red tape or unnecessary process. Not only will you get to work with a great team, you’ll also enjoy flexible hours, the flexibility to work from home when needed, private medical and a relaxed dress code.\r\n\r\nREQUIRED SKILLS:\r\n Ability to pick up a new technology quickly and deliver features in a highly agile manner.\r\n Experience writing functional Scala in a production grade system. (Not a Java developer writing OO in Scala).\r\n To have used Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\n A clear and practical understanding of how Hive works, which includes running Hive on Tez.\r\n Practical experience using Debian flavoured Linux distributions.\r\n Familiarity with event driven development and architecture.\r\n Used Docker containers to deploy your systems.\r\n Ability to navigate the administration of an HDP cluster on AWS.\r\n Able to index millions of documents from Hadoop into Elasticsearch.\r\n Work with various messaging systems, such as Kafka and RabbitMQ.\r\n Able to aggregate data using Apache Kylin Cube.\r\n Can pick up Python if you have not used it previously.\r\n Operate and deploy to a Kubernetes cluster on AWS.\r\n Understand basic concepts about mortgage backed securities.\r\n\r\nIf you qualify for this role, please email your CV directly to:\r\n Gary Silbermann\r\n gary@acuityconsultants.co.za\r\n 021 801 5001\r\n\r\nIf you have not had a response to your application within 14 days please consider your application to be unsuccessful."},{"JobTitle":" Data Scientists","Url":"https://www.indeed.co.za/rc/clk?jk=cf9f5740445723a4&fccid=830659e7e0144aca&vjs=3","Salary":null,"Source":" Knowledge Integration Dynamics","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\n\r\n\r\n\r\nWe are looking for Data Scientists that will help our clients discover the information hidden in vast amounts of data, and help them make smarter data driven decisions. Your primary focus will be in applying data mining techniques, doing statistical analysis, and building high quality prediction systems.\r\n Your responsibilities will include:\r\nSelecting features, building and optimizing classifiers using machine learning techniques\r\nData mining using state-of-the-art methods\r\nExtending a company’s data with third party sources of information when needed\r\nEnhancing data collection procedures to include information that is relevant for building analytic systems\r\nProcessing, cleansing, and verifying the integrity of data used for analysis\r\nDoing ad-hoc analysis and presenting results in a clear manner\r\nCreating automated anomaly detection systems and constant tracking of its performance\r\n\r\nEducation/Degree:\r\n A post-graduate degree in a quantitative field such as Statistics, Mathematics, Physics or Econometrics with solid Information Technology experience\r\n OR A post-graduate degree in a IT field with solid quantitative analysis experience\r\n\r\nSkills & Experience Required:\r\n Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision trees, Ridge regression, MARS regression, etc.\r\n Experience with common data science toolkits such as R, Python or Scala. Excellence in at least one of these is highly desirable\r\n Great communication skills and the ability to present data science methodologies to a business audience\r\n Experience with data visualisation tools, such as ggplot, plotly or seaborn\r\n Proficiency in using query languages such as SQL, Hive or Pig\r\n Good applied statistics skills, such as distributions, statistical testing, regression, etc.\r\n Good scripting and programming skills\r\n Data-oriented personality\r\n Experience in a dashboard technology, such as Power BI or Tableau\r\n\r\nWhen applying for jobs, ensure that you have the minimum job requirements. Applicants for positions in South Africa must have a valid South African work permit or be a permanent resident. If you are not in the mentioned location of any of the jobs, please note your relocation plans in all applications for jobs and correspondence. \r\nPlease e-mail a word copy of your CV and mention the jobs you are interested in to \r\n\r\n While we would really like to respond to every application, should you not be contacted for this position within 10 working days please consider your application unsuccessful."},{"JobTitle":" Big Data Developer (CPT)","Url":"https://www.indeed.co.za/rc/clk?jk=960548b153c061f5&fccid=6ab9a95a1ff7d948&vjs=3","Salary":null,"Source":" Parvana Strategic Sourcing","Age":"30+ days ago","Location":"Cape Town, Western Cape","Description":"\r\n\r\n\r\nCape Town, Western Cape\r\n\r\n\r\nPermanent\r\n\r\n\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\n\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\n\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk \r\n\r\n\r\n\r\nBig Data Developer (CPT) (New) | (1002333)\r\n\r\n[Permanent | Competitive Salary | Cape Town]\r\n\r\n\r\nClient Background: Our client develops and supports software and data solutions across a variety of industries. \r\n They want you to get ahead of the market and stay there. They offer a combination of plug and play products that can be integrated with existing systems and processes and can also be customised to client needs. \r\n Their capabilities extend to big data engineering and bespoke software development, solutions are available as both cloud-based and hosted.\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk"},{"JobTitle":" Senior Specialist: Support Analyst","Url":"https://www.indeed.co.za/rc/clk?jk=acbd902ab746d6c9&fccid=374d720d3973ca1c&vjs=3","Salary":null,"Source":" Vodafone","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\n\r\nVodacom is a Leading African Mobile communication company providing wider range of communication services including mobile voice, messaging, data and converged services to over 73.6 million customers. From our roots in South Africa, we have grown our mobile network business to include operations in Tanzania, DRC, Mozambique and Lesotho. The mobile networks cover a total population of approximately 200 million people. Through Vodacom Business Africa (VBA) we also offer business managed services to enterprises in over 40 countries across the continent. Vodafone is the majority shareholder of Vodacom and has a 65% share.\r\n\r\n\r\n\r\n\r\n\r\nWere at our best when we lead and over the past 20 years, as the Company that pioneered mobile in South Africa, Vodacom has achieved a remarkable list of firsts. Were immensely proud to be a leader in our field and are 100% committed to continue trailblazing.\r\n\r\n\r\n\r\n\r\nWe employ individuals who are as passionate about customers as we are. We are truly Customer Obsessed which means that we are passionate about exceeding customer expectations; work relentlessly to really understand the customer; look at decisions through the customers eyes and take personal accountability for the customer experience.\r\n\r\n\r\n\r\n\r\nWe have the below vacancy available in our Organisation:\r\n\r\n\r\n\r\n\r\n\r\nThe G Band Senior Specialist: Support Analyst role is based within Local Technology\r\n\r\n\r\n\r\n\r\n\r\nThe role of the Senior Specialist: Support Analyst is to be responsible for and ensuring of data import and processing into the BDP. Installation of CDH repository, OS level configuration, new type of I/O compression library cluster, and Hadoop installation &support including the Cloudera manager, adding new nodes and services to the cluster\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nYour responsibilities will include:\r\n\r\n\r\n Configure and Maintain Name Node/ Resource manager High Availability (HA), including Hive, Impala\r\n\r\n\r\n\r\n Manage cluster support by rebalancing the cluster, setting up alerting for disk fill, and YARN resource management;\r\n\r\n\r\n\r\nConfigure HDFS ACLs, Sentry, Hue user authorization and authentication;\r\n\r\n\r\n\r\nContinuous benchmarking of the cluster s operational metrics, test systems configuration for operational efficiency;\r\n\r\n\r\n\r\nResolve all logged incidences and service requests (SR) Troubleshoot and resolve errors/warnings in Cloudera Manager and application delays. Resolve performance problems/errors in operations\r\n\r\n\r\n Provide guidance and decisive technical leadership\r\n\r\n\r\n Ensure the BDP cluster is secured from potential vulnerabilities\r\n\r\n\r\n Provided a leadership role with key stakeholders for optimal deliverance of use cases\r\n\r\n\r\n Drive DevOps way of working\r\n\r\n\r\n Expert level experience with using Spark, Yarn, Hive and Oozie\r\n\r\n\r\n Python and Scala programming ability will be an advantage\r\n\r\n\r\n Working knowledge of HBase, Kafka, Cassandra and Flume\r\n\r\n\r\n Ability to work standby and overtime when required\r\n\r\n\r\n Expert knowledge of NiFi, Sqoop and Flume will be an added advantage.\r\n\r\n\r\n\r\n\r\n\r\n\r\nKey accountabilities and decision ownership:\r\n\r\n\r\n\r\nExtensive experience in designing, building and managing BDP applications in ingesting and storing large amounts of data in a Hadoop/HDFS ecosystem;\r\n Extensive experience with performance tuning applications on Hadoop/YARN and configuring Hadoop/YARN systems to maximise performance;\r\n Extensive experience in installing, testing, configuring BDP ecosystems\r\n Extensive experience is solving complex requests or service impacting incidents within the BDP cluster\r\n Experience working in a multi tenancy Hadoop environment\r\n\r\n\r\n\r\n\r\n\r\nCore competencies, knowledge and experience:\r\n\r\n Hadoop,\r\n\r\n YARN,\r\n Linux,\r\n HDFS\r\n CDH\r\n Scripting/Java/Python/R\r\n\r\n\r\n\r\n The ideal candidate for this role will have:\r\n Matric/ Grade 12 qualification\r\n 3 year completed Ndip/Degree in Information Technology or equivalent\r\n At least 5-8 years IT experience (Telecoms or Fixed mobile advantageous)\r\n CCA131 certification (desirable)\r\n Knowledge of Big Data (essential)\r\n Agile Methodology(desirable)\r\n Hadoop or YARN or Linux or HDFS or CDH or Scripting/Java/Python/R (essential)\r\n\r\n\r\n\r\nIn addition to the details listed above, the ideal candidate will have an in-depth knowledge and understanding\r\n\r\n\r\n\r\n Availability and optimal functioning of the BDP cluster ecosystems\r\n Servicing incidents and service requests in a timely manner\r\n Adjustment to the Agile environment\r\n\r\n\r\n\r\nThe base location for this role is Midrand, Commercial Park\r\n\r\n\r\n\r\nThe Companys approved Employment Equity Plan and Targets will be considered as part of the recruitment process. As an Equal Opportunities employer, we actively encourage and welcome people with various disabilities to apply.\r\n\r\n\r\n\r\nVodacom is committed to an organisational culture that recognises, appreciates and values diversity inclusion."},{"JobTitle":" Senior Data Scientist - Advanced Analytics","Url":"https://www.indeed.co.za/rc/clk?jk=4a8d202c8820b1d7&fccid=6576e7250aa78c3c&vjs=3","Salary":null,"Source":" McKinsey & Company","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\n\r\n\r\n\r\nQualifications\r\n\r\nBachelor’s degree in quantitative field like Computer Science, Engineering, Statistics, Mathematics or related field required; Advanced degree is a plus\r\n 3-5 years of hands-on mathematical modelling experience in business environment\r\n Programming (focus on Machine Learning) in R and/or Python (must), SPSS, SAS, Ruby, Hadoop (valued)\r\n Data treatment/Data mining SQL, AWK, Access, Spark, Excel (valued\r\n Advanced knowledge of statistical and machine learning techniques (regression, decision trees, clustering, neural networks, etc.)\r\n Proven experience in working with large datasets and relational databases (SQL)\r\n Distinctive communications skills and ability to communicate analytical and technical content in an easy to understand way\r\n Intellectual curiosity, along with excellent problem-solving and quantitative skills, including the ability to disaggregate issues, identify root causes and recommend solutions\r\n Proven leadership with the ability to inspire others, build strong relationships, and create true followership, result-driven achievers\r\n Strong people skills, team-orientation, and a professional attitude\r\n Experience and interest in RDBMS systems (MySQL, IBM DB2, Oracle Database, etc.), cloud (AWS, Azure, Google Cloud Platform) and big data technologies (e.g. Hadoop, Hive, Impala, Spark, NoSQL DBs) is preferable (reword?)\r\n Experience in data extraction, transformation, cleaning, and validation\r\n Experience implementing advanced analytics / data science models into a production environment is a plus\r\n\r\n\r\n\r\nWho You'll Work With\r\n\r\nYou will join McKinsey’s Africa Analytics Practice, based in Johannesburg. Our Advanced Analytics teams bring the latest analytical techniques plus a deep understanding of industry dynamics and corporate functions to help clients create the most value from data in order to unlock the strategic CEO agenda.\r\n\r\nYour role will entail extensive interactions with our global analytics community; partnering with generalist consultants, clients and other colleagues. \r\n\r\n\r\n\r\nWhat You'll Do\r\n\r\nYou will be involved in specific engagements involving the building of new and bespoke advanced analytics solutions for clients.\r\n\r\nLeveraging your advanced data and analytics skills, you will create innovative approaches to answer our clients’ most relevant questions. You will prepare complex data analyses and models that help solve client problems and deliver significant measurable impact. You will have a strong understanding of the design and development process of data analytics tools and experience working with data engineers and other data scientists, both from McKinsey and clients."},{"JobTitle":" Hadoop developer","Url":"https://www.indeed.co.za/company/Prorek/jobs/Hadoop-Developer-002d963619151341?fccid=6c17122040c57e81&vjs=3","Salary":null,"Source":" Prorek Solutions","Age":"12 days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nContract\r\n\r\nCandidate should have hands on experience in Hadoop along with ecosystem (including HDFS, Spark, Sqoop, Flume, Hive, Impala, PIG, MapReduce/YARN)\r\n\r\n- Experience working on Unix / Linux environment, as well as Windows environment\r\nMinimum 3 - 5 years required.\r\nPlease send resume to hr@prorek dot net\r\n\r\nJob Type: Contract\r\n\r\nExperience:\r\nHadoop: 3 years (Preferred)"},{"JobTitle":" Big Data Developer (JHB)","Url":"https://www.indeed.co.za/rc/clk?jk=dc2adc6e2281aaed&fccid=6ab9a95a1ff7d948&vjs=3","Salary":null,"Source":" Parvana Strategic Sourcing","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nPermanent\r\n\r\n\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\n\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\n\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk \r\n\r\n\r\n\r\nBig Data Developer (JHB) (New) | (1002332)\r\n\r\n[Permanent | Competitive Salary | Johannesburg]\r\n\r\n\r\nClient Background: Our client develops and supports software and data solutions across a variety of industries. \r\n They want you to get ahead of the market and stay there. They offer a combination of plug and play products that can be integrated with existing systems and processes and can also be customised to client needs. \r\n Their capabilities extend to big data engineering and bespoke software development, solutions are available as both cloud-based and hosted.\r\n\r\n\r\nResponsibilities: Building and operating a content management platform for a high-profile big data project that promises to revolutionise an area of finance by providing unprecedented market insight in a timely manner.\r\nWorking closely within a team of developers distributed in London, Johannesburg, Cape Town and New Zealand.\r\nBuilding and operating an ingestion and analytics platform that collects frequently changing data and exposing the normalised and aggregated data via APIs to our client’s customers. This will include data cleansing, aggregation, financial computations.\r\n\r\nSkills & Experience: Must be able to pick up a new technology quickly and deliver features in a highly agile manner.\r\nExperience writing functional Scala in a production grade system. You are not a Java developer writing OO in Scala.\r\nPrevious experience using Apache Spark SQL in a production system using Scala with YARN as the resource manager.\r\nClear and practical understanding of how Hive works, which includes running Hive on Tez.\r\nPractical experience using Debian flavoured Linux distributions.\r\nFamiliarity with event driven development and architectures.\r\nPrevious experience using Docker containers to deploy your systems.\r\nAbility to easily navigate the administration of an HDP cluster on AWS.\r\nMust be able to index millions of documents from Hadoop into Elasticsearch.\r\nAbility to work with various messaging systems, such as Kafka and RabbitMQ.\r\nMust be able to aggregate data using Apache Kylin Cube.\r\nMust be able to easily pick up Python if you have not used it previously.\r\nAbility to operate and deploy to a Kubernetes cluster on AWS.\r\nMust be able to understand basic concepts about mortgage backed securities.\r\n\r\nTo apply use the application tool or send us an email to recruitment@parvana.co.uk"},{"JobTitle":null,"Url":"https://www.indeed.co.za","Salary":" R80 000 - R90 000 a year","Source":" PRR Recruitment Services","Age":"30+ days ago","Location":null,"Description":null},{"JobTitle":" ETL Developer","Url":"https://www.indeed.co.za/rc/clk?jk=3723d2f4ad7de8b5&fccid=456208dd78cb7bb1&vjs=3","Salary":null,"Source":" Altron","Age":"30+ days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nContract\r\n\r\n\r\n\r\nThe ETL Developer is responsible for the design, build and deployment of a project's ETL components. A typical ETL effort usually involves multiple ETL Developers developing the Informatica mappings, executing them in native and/or pushdown mode and validating the results. These tasks involve data ingestion into Hadoop, Data Integration and Data Quality on Hadoop and Data extraction from Hadoop to external systems. \r\n\r\n\r\nJob Requirements:\r\n\r\n\r\nResponsibilities:\r\nUses the Informatica DI platform to extract data from external sources and ingest them into Hadoop.\r\nUses the Informatica Developer to perform DI operations on data within Hadoop.\r\nUses the Informatica Developer to perform DQ operations on data within Hadoop.\r\nIntegrates the Hadoop ecosystem (i.e., services such as HDFS, Hive and HBase) with other non Hadoop enterprise level technologies such as ERP and RDBMS.\r\nDevelops Data Integration workflows and load processes.\r\nEnsures adherence to locally defined standards for all developed components.\r\nPerforms data analysis for both Source and Target tables/columns including those on HDFS.\r\nProvides technical documentation of Source and Target mappings.\r\nParticipates in design and development reviews.\r\nWorks with System owners to resolve source data issues and refine transformation rules.\r\nEnsures performance metrics are met and tracked.\r\nWrites and maintains unit tests.\r\nConduct QA Reviews."},{"JobTitle":" Senior Application Developer","Url":"https://www.indeed.co.za/rc/clk?jk=6dcf0988599a5242&fccid=a4bf7e93f92792dd&vjs=3","Salary":" R60 000 - R70 000 a year","Source":" PRR Recruitment Services","Age":"30+ days ago","Location":"Cape Town, Western Cape","Description":"\r\n\r\n\r\nCape Town, Western Cape\r\n\r\n\r\nR60 000 - R70 000 a year\r\n\r\n\r\nLooking for a skilled and versatile full-stack developer to join the team in either of our South African offices. The role involves building applications for a high profile Big Data project that promises to revolutionize an area of finance.\r\nHave great working knowledge of at least one JavaScript web application framework (e.g. Angular, React, Vue, Aurelia). We use Angular so you should at least be familiar with its concepts.\r\nExperience with HTML5, CSS3, SVG, Web Standards, Progressive Enhancement.\r\nExperience with modern web development libraries and tools (e.g. Webpack, Typescript, D3.js, Babel, SASS etc).\r\nProven ability in at least one server-side MVC framework (e.g. Java Spark, .NET Core MVC, Node js).\r\nDesign capabilities using OO and SOLID techniques.\r\nGeneral technology problem solving skills to a high level.\r\n\r\nTo succeed in the role, you’ll need\r\nTo use Docker to deploy applications and run them on our Kubernetes clusters\r\nTo extensively use GIT\r\nTo understand NoSQL databases. At the moment our tech stack includes Elasticsearch, Hive and HBase.\r\nTo be able to write performant complex queries over Postgres and Redshift"},{"JobTitle":" Data Scientist","Url":"https://www.indeed.co.za/rc/clk?jk=79b07c24bea88b88&fccid=8de4e5eeef794daa&vjs=3","Salary":" R550 - R650 a week","Source":" Tipp Focus","Age":"30+ days ago","Location":"Midrand, Gauteng","Description":"\r\n\r\n\r\nMidrand, Gauteng\r\n\r\n\r\nR550 - R650 a week\r\nThis position will provide complete application lifecycle development, deployment, and operations support for Big Data solutions and infrastructure. ? In this role, you will collaborate with product owners, data scientists, solutions engineers, and business analysts to facilitate the development, automation, and seamless delivery of analytics solutions into Big Data clusters. \r\n\r\n Strong background in mathematics and have very good analytical and problem solving skills. \r\n\r\n Languages Python, Scala, SQL, Java, PL/SQL Web Technologies Web Service, SOAP, Rest web services, JSP \r\nBig Data Eco System \r\nHDFS, Spark, Yarn, Map Reduce, Hive, Pig, Sqoop, ZooKeeper, Kafka, Oozie, Hue, Impala, Flume. Scripting Languages HTML, JavaScript, CSS, XML and Ajax Machine Learning R, SAS, Python, SKLearn, MATLAB, Octave, Spark ML No SQL Databases Cassandra, HBase, MongoDB, Vertica Cloud AWS, EC2, S3, EMR, Azure Operating System Windows, Linux and Unix \r\nBI/DWH/ETL Tools Informatica 9.5/9.1/8.6, Tableau, Cognos DBMS / RDBMS Oracle 12c/11g, SQL Server 2014, DB2, Teradata 14/12, AWS Redshift \r\nIDEs \r\nEclipse, Jupiter Notebooks, Microsoft Visual Studio, Flex Builder, Spyder, TOAD, NetBeans, PL/SQL Developer, Putty, Squirrel SQL Version Control SVN, CVS, Git, and Rational Clear Case \r\nTools FileZilla, JUnit, Splunk, HP ALM, Clear Quest, Rally, Jira \r\n\r\nActivities:\r\n Installing, configuring and using ecosystem components like Hadoop Map Reduce, Spark, Hive, Sqoop, Pig, HDFS, HBase, Cassandra, ZooKeeper, Oozie, Hue, Impala and Flume. ? Strong experience in Data Warehousing and ETL using Informatica Power Center. ? Very good experience in analyzing the data and reporting it using data visualization tools such as Tableau, Cognos, MicroStrategy. ? Good understanding of Hadoop architecture & various components of HDFS/ Yarn. ? Experience in Data Mining, Data Analysis, Data Migration, Data Validation, Data Cleansing, Data Verification and identifying Data Mismatch. ? Experience in Machine Learning solving classification and clustering \r\nproblems. ? Interpreting the results of statistical and predictive experiments and regression analysis. ? Importing and exporting data using Sqoop from HDFS to Relational Database Systems and vice-versa. ? Experience with CSV, JSON, Sequence files, AVRO, Parquet, RC file formats. ? Experience with Spark Context, SQL Context, Spark-SQL, Data Frames, Pair RDD's, transformations, actions in Spark. ? Expert in creating PIG and HIVE UDFs using java in order to analyze data sets. ? Experience using HBase, Cassandra, MongoDB No-SQL databases for real time low latency queries. ? Experience with Spark programs, Hive queries, pig Latin scripts, and MapReduce programs for data analysis and to process the data and loading into databases for visualization. ? Extensively worked on data extraction, Transformation and loading (ETL) data from various sources like Oracle, SQL Server and flat files and loaded into DWH for reporting and data analysis. ? Well versed in developing the complex SQL queries, multiple table joins, analytical functions, regular expressions etc. ? Experience in preparing and executing test plan and test cases after software development. ? In depth understanding of data structures and algorithms. ? Experience working with both the Waterfall and Agile methodologies. ? Experience in giving training and guiding new team members in the Project. ? Experience coding and testing the Standardization, Normalization, Load, Extract and AVRO models to filter/massage the data and its validation. ? Proficient in HealthCare, Education, Retail and Banking Domains. ? Very good experience in customer specification study, requirements gathering, system architectural design and turning the requirements into final product. ? Experience in interacting with customers and working at client locations for real time field testing of products and services. ? Ability to work effectively with associates at all levels within the organization."},{"JobTitle":" Java Dev - Gloucester - DV","Url":"https://www.indeed.co.za/rc/clk?jk=8157ca075edb97a8&fccid=33e11e8479be6683&vjs=3","Salary":null,"Source":" BAE Systems Applied Intelligence","Age":"30+ days ago","Location":"East London, Eastern Cape","Description":"\r\n\r\n\r\nEast London, Eastern Cape\r\n\r\n\r\n\r\nWe are looking for experienced developers who are motivated to make a difference in our connected world. BAE Systems Applied Intelligence continues to innovate and win new business in our core business of enabling our client's missions of protecting people and critical national infrastructure. We need your talent to keep growing our products and succeed in our ambitions. Our backend Java developers based in Guildford are constructing information processing systems that work with data on a massive scale working in collaboration with teams that make specialist equipment. \r\n\r\nSuccessful applicants will be the leaders and influencers joining the product groups with years of experience as a Java developer alongside our graduate team expansion and shall join the existing leadership group. The solution space has led our products to the use of the newest versions of Kafka, ElasticSearch, Kubernetes, Impala, Hive and Spark; we cross-train Java developers onto Scala. Our mastery of these technologies has enabled us to extend some of these core technologies in order to realise significant storage and processing efficiencies. For interacting with the front end developers we use Swagger defined microservices. \r\n\r\nFor the future we're looking for the products to standardise on GraphQL as already implemented in the NetReveal suite of products, and to standardise the CI/CD pipelines we already have using Cloud for scale out build and on-demand integration testing. \r\n\r\nThe Global Engineering Platform is indicative of our investment in software engineering. It provides all engineers with BitBucket (git), Jira, Confluence, Jama, Rocket.Chat, Bamboo (though many teams use Jenkins), Nexus, Cruicible & Fisheye, Zephyr and Cucumber. All software engineers have access to online training and books from Pluralsight and Skillsoft. \r\n\r\nWe are passionate about our software engineering and helping our customers; we work hard, support each other and are serious about our standards, tooling and training. We use our tailored version of Scrum as taught internally by our Agile Coaches and have high expectations of ourselves and each other. We do believe in flexibility especially for engineers with dependents, including part-time roles, as fair and ethical behaviour is critically important to us. \r\n\r\nRequirements \r\nExpert in backend Java development \r\nTeam player and good communicator \r\nExperienced code reviewer \r\nProblem solving as part of software design and development \r\nDesigning and debugging distributed processing \r\nSound understanding of working as part of a software engineering team, processes, tooling such as git \r\nTeaching/coaching junior engineers \r\n\r\nAbout BAE Systems Applied Intelligence \r\n\r\nWe use our intelligence-led insights to help defend Governments, Nations and Societies from cyber-attacks and financial crime. Our customers depend on our evolving capabilities to help them safely grow their organisations. Our unprecedented access to threat intelligence, world-leading analysts and market-leading technology means we can help them to adapt, evolve and stay ahead of the criminals. \r\n\r\nDivision overview: Capabilities \r\n\r\nAt BAE Systems Applied Intelligence, we pride ourselves in being a leader in the cyber defence industry, and Capabilities is the engine that keeps the business moving forward. It is the largest area of Applied Intelligence, containing our Engineering, Consulting and Project Management teams that design and implement the defence solutions and digital transformation projects that make us a globally recognised brand in both the public and private sector. \r\n\r\nAs a member of the Capabilities team, you will be creating and managing the solutions that earn us our place in an ever changing digital world. We all have a role to play in defending our clients, and this is yours.\r\n\r\n\r\n\r\nDiversity and inclusion are integral to the success of BAE Systems Applied Intelligence. We are proud to have an organisational culture where employees with varying perspectives, skills, life experiences and backgrounds – the best and brightest minds – can work together to achieve excellence and realise individual and organisational potential. We also welcome discussions about flexible working."},{"JobTitle":" Experienced Bee Hive Remover Wanted","Url":"https://www.indeed.co.za/rc/clk?jk=dc5aab3631ddc187&fccid=7854245953eb4df9&vjs=3","Salary":null,"Source":" Gumtree.co.za","Age":"2 days ago","Location":"Johannesburg, Gauteng","Description":"\r\n\r\n\r\nJohannesburg, Gauteng\r\n\r\n\r\nContract\r\nExperienced Bee remover. Have you removed beehives before? we are currently looking for an individual with own transport preferably a Bakkie to do bee removals in and around Gauteng. please go to our web site to get an overview of what we do. \r\n\r\n we look forward to hearing from you. www.beealive.co.za"},{"JobTitle":" BI Developer","Url":"https://www.indeed.co.za/rc/clk?jk=f892b7708270c06f&fccid=2429f7b96dfc0010&vjs=3","Salary":null,"Source":" Dynamic Visual Technologies (DVT)","Age":"30+ days ago","Location":"East London, Eastern Cape","Description":"\r\n\r\n\r\nEast London, Eastern Cape\r\n\r\n\r\n\r\n\r\n\r\nOur Data and Analytics Team\r\n\r\nWe are a growing team of passionate data and analytics professionals working at DVT in Pretoria, Johannesburg, Cape Town and Durban, focussed on building business value for our customers. Our consulting and project engagements, both locally and internationally, range across a wide variety of sectors including manufacturing, retail, marketing and financial services, providing challenging diversity of experience to our consultants.\r\n\r\n\r\nOur team includes junior, intermediate and senior consultants and developers as well as some of the best analytics strategists. If you are passionate about the power of data and analytics, Big Data, machine learning or data science we’re looking for you! Apply now.\r\n\r\n\r\nWe offer the opportunity for you to work in Agile teams on high-profile customer engagements where information is used to craft digital business. Work on solutions to take our enterprise customers on an analytics maturity journey, ranging from formulating foundational data capability, information connectivity to leveraging high order analytics in a dynamic fashion.\r\n\r\n\r\n\r\nThe skills we are seeking from applicants and developing in our team members include:\r\n\r\n\r\nMicrosoft SSIS, SSAS (Multidimensional and Tabular), SSRS \r\nMicrosoft PowerBI\r\nPowershell\r\nAzure SQL DB, Azure Datawarehouse, Azure Data Lake, Azure Data Factory, Revolution R, HD Insights, Polybase\r\nApache projects including SQOOP, HADOOP, HIVE, MAHOUT, SPARK \r\nGoogle Data Studio, Big Query\r\nBusiness consulting leveraging analytics\r\n\r\n\r\nYour skills will deepen as you work on our high-profile engagements enabling Digital Business. DVT Data and Analytics is where you can grow your career in data science - as you grow in seniority, become a Senior Data and Analytics Consultant or Analytics Solutions Specialist.\r\n\r\n\r\nWork with customers to enable solutions that transform business.\r\n\r\n\r\nIf you “get” the power of data and analytics in modern business and want to be one of our guru’s helping customers realize true digital capability, we want to talk to you about joining our team.\r\n\r\n\r\n\r\n\r\n\r\nWhy join our Data and Analytics Team\r\nLocal and International client projects \r\nAbove market remuneration and performance bonuses\r\nMaternity and paternity benefits\r\nCompany paid group life and disability cover\r\n20 days annual leave\r\nGenerous contribution towards training and certification\r\nHigh spec developer machines\r\nParticipate in the digital business revolution\r\nA true Data and Analytics culture in a dedicated division.\r\n\r\n\r\n\r\n\r\n\r\nAbout DVT\r\n\r\nIt all started with a passion for creating high-impact business software that we and customers can see working in production as quickly as possible.\r\n\r\n\r\nSince 1999 DVT has been on the forefront of bespoke business software solutions for some of the largest and most dynamic companies in South Africa and abroad. Starting out as custom business software developers in Cape Town, we’ve grown to become one of South Africa’s leading software and services providers with delivery centres in Johannesburg, Cape Town and Durban and have recently cut the ribbon on our office in London.\r\n\r\n\r\nDelivering great software requires much more than good programming, design and architecture. Today, with more than 700 staff, we help clients with everything needed to get quality software in production faster. This includes Agile and Lean consulting and training, cutting code, software testing, IT consulting and architecture, business and process analysis, project management, business intelligence as well as product implementation.\r\n\r\n\r\nWe also run the largest specialist software testing centre in Cape Town focused on testing automation.\r\n\r\n\r\n\r\n\r\nOur Culture\r\n\r\nWe are known for our people centric approach, unwavering business integrity and working in an Agile approach with high quality always top of mind.\r\n\r\n\r\n\r\nAt DVT we live by a set of values. Our values say who we are. They guide us in making a positive impact with our clients, business partners, the communities we operate in and amongst ourselves.\r\n\r\n\r\nOur Values\r\n\r\n\r\n\r\n\r\nHow we work\r\n\r\nAt DVT you will have the opportunity to work in an extraordinary wide range of business domains and technologies. Some of our projects happen in of our offices in Agile teams or on-site at clients in Agile teams. Either way, you will work with some of the best developers in South Africa working on software solutions for leading companies. Some of our team service international clients and utilise business communication technologies such as Skype stand-ups."}]